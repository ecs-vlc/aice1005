%Master File:lectures.tex

\lesson{Dynamic Programming}

\vspace{-2cm}
\begin{center}
  \includegraphics[height=10cm]{dynamic_programming_ans}
\end{center}
\keywords{Dynamic programming, line breaking, edit distance, Dijkstra, TSP}

%%%%%%%%%%%%%%%%%%%%%%% Next Slide %%%%%%%%%%%%%%%%%%%%%%%
\renewcommand{\Outline}{%
\begin{slide}
\section[1]{Outline}

\begin{minipage}{12cm}
  \begin{enumerate}\squeeze
    \outlineitem{Dynamic Programming}{dp}
    \outlineitem{Applications}{applic}
    \begin{itemize}
    \item \toplink{paragraphStuff}{Line Breaks}
    \item \toplink{other}{Edit Distance}
    \item \toplink{dijkstra}{Dijkstra's Algorithm}
    \end{itemize}
    \outlineitem{Limitation}{limitation}
  \end{enumerate}
\end{minipage}\hfill
\begin{minipage}{10cm}
  \includegraphics[width=10cm]{dynamic_programming_ans}
\end{minipage}
\end{slide}
\addtocounter{outlineitem}{1}
}

\setcounter{outlineitem}{1}
\Outline % Dynamic Programming
\toptarget{firstoutline}
%%%%%%%%%%%%%%%%%%%%%%% Next Slide %%%%%%%%%%%%%%%%%%%%%%%


\begin{slide}
\section{Dynamic Programming}

\begin{PauseHighLight}
  \begin{itemize}
  \item One of the most powerful strategies for solving optimisation
    problems is \emph{dynamic programming}\pause
    \begin{itemize}\squeeze
    \item Build a set of optimal partial solutions\pause
    \item Increase the size of the partial solutions until you have a
      full solution\pause
    \item Each step uses the set of optimal partial solutions found in
      the previous step\pause
    \end{itemize}
  \item Developed by Richard Bellman in the early 1950's\pause
  \item The name is unfortunate as it doesn't have much to do with
    programming\pause 
  \end{itemize}
\end{PauseHighLight}

\end{slide}

%%%%%%%%%%%%%%%%%%%%%%% Next Slide %%%%%%%%%%%%%%%%%%%%%%%

\begin{slide}
\section[-2]{A Toy Problem}

\begin{PauseHighLight}
  \begin{itemize}
  \item Consider the problem of find a minimum cost path from point
    (0,0) to (8,0) on the lattice\pause
    \begin{center}\color{TextColor}
      \includegraphics[height=8cm]{lattice_prob.mps}
    \end{center}\color{TwoColor}
  \item The costs of traversing each link is shown in red\pause
  \item The cost of a path is the sum of weights on each link\pause
  \end{itemize}
\end{PauseHighLight}

\end{slide}

%%%%%%%%%%%%%%%%%%%%%%% Next Slide %%%%%%%%%%%%%%%%%%%%%%%

\begin{slide}
\section[-0.5]{Brute Force}

\begin{PauseHighLight}
  \begin{itemize}
  \item The obvious brute force strategy is to try every path\pause
  \item For a problem with $n$ steps we require $n/2$ to be diagonally
    up and $n/2$ to be diagonally down\pause
  \item The total number of paths is
    \begin{align*}
      \binom{n}{n/2} \approx \sqrt{\frac{2}{\pi\,n}}\; 2^n\pause
    \end{align*}
  \item For the problem shown above with $n=8$ there are 70
    paths\pause
  \item For a problem with $n=100$ there are $1.01\times10^{29}$
    paths\pause
  \end{itemize}
\end{PauseHighLight}

\end{slide}

%%%%%%%%%%%%%%%%%%%%%%% Next Slide %%%%%%%%%%%%%%%%%%%%%%%

\begin{slide}
\section[-2]{Building a Solution}

\begin{PauseHighLight}
  \begin{itemize}
  \item We can solve this problem efficiently using dynamic programming
    by considering optimal paths of shorter length\pause
  \item Let $c_{(i,j)}$ denote the cost of the optimal path to node
    $(i,j)$\pause
  \item We denote the weights between two points on the lattice by
    $w_{(i,j)(i+1,j\pm1)}$
    \begin{center}
      \includegraphics[height=4cm]{lattice_prob_notation}\pause
    \end{center}
  \item Clearly $c_{(0,0)}=0$\pause
  \end{itemize}
\end{PauseHighLight}

\end{slide}

%%%%%%%%%%%%%%%%%%%%%%% Next Slide %%%%%%%%%%%%%%%%%%%%%%%

\begin{slide}
\section[-0.5]{Forward Algorithm}

\begin{PauseHighLight}
  \begin{itemize}
  \item Suppose we know the optimal costs for all the edge in column
    $i$\pause
  \item Our task is to find the optimal cost at column $i+1$\pause
  \item If we consider the sites in the lattice then the optimal cost
    will be
    \begin{align*}
      c_{(i+1,j)} = \min\!\left( \strut c_{(i,j+1)}+w_{(i,j+1)(i+1,j)},\,\,
        c_{(i,j-1)}+w_{(i,j-1)(i+1,j)}\right)\pause
    \end{align*}
  \item This is the defining equation in dynamic programming\pause
  \item We have to treat the boundary sites specially, but this is just
    book-keeping\pause
  \end{itemize}
\end{PauseHighLight}

\end{slide}

%%%%%%%%%%%%%%%%%%%%%%% Next Slide %%%%%%%%%%%%%%%%%%%%%%%

\begin{slide}
\section[-2]{Example}
\pb
\begin{center}
  \multiinclude[graphics={height=15cm}]{dynamicprog}\pause
\end{center}
\end{slide}

%%%%%%%%%%%%%%%%%%%%%%% Next Slide %%%%%%%%%%%%%%%%%%%%%%%

\begin{slide}
\section{Backward Algorithm}

\begin{PauseHighLight}
  \begin{itemize}
  \item Having found the optimal costs $c_{(i,j)}$ we can find the
    optimal path starting from $(n,0)$\pause
  \item At each step we have a choice of going up or down\pause
  \item We choose the direction which satisfies the constraint
    \begin{align*}
      c_{(i,j)} = c_{(i-1,j\pm1)}+w_{(i-1,j\pm1)(i,j)}\pause
    \end{align*}
  \item If both directions satisfy the constraint we have more than one
    optimal path\pause
  \end{itemize}
\end{PauseHighLight}

\end{slide}

%%%%%%%%%%%%%%%%%%%%%%% Next Slide %%%%%%%%%%%%%%%%%%%%%%%

\begin{slide}
\section{Time Complexity}

\begin{PauseHighLight}
  \begin{itemize}
  \item In our dynamic programming solution we had to compute the cost
    $c_{(i,j)}$ at each lattice point\pause
  \item There were $(n+1)^2$ lattice point\pause
  \item It took constant time to compute each cost so the total time to
    perform the forward algorithm was $\Theta(n^2)$\pause
  \item The time complexity of the backward algorithm was
    $\Theta(n)$\pause
  \item This compares with $\exp(\Theta(n))$ for the brute force
    algorithm\pause 
  \end{itemize}
\end{PauseHighLight}

\end{slide}

%%%%%%%%%%%%%%%%%%%%%%% Next Slide %%%%%%%%%%%%%%%%%%%%%%%
\Outline % Applications
%%%%%%%%%%%%%%%%%%%%%%% Next Slide %%%%%%%%%%%%%%%%%%%%%%%

\begin{slide}
\section{Applications of Dynamic Programming}

\begin{PauseHighLight}
  \begin{itemize}
  \item Dynamic programming is used in a vast number of applications
    \begin{itemize}
    \item String matching algorithms
    \item Shape matching in images
    \item Dynamical time-warping in speech
    \item Hidden Markov Models in machine learning\pause
    \end{itemize}
  \item Unlike greedy algorithms the idea is readily extended to many
    different applications\pause
  \end{itemize}
\end{PauseHighLight}

\end{slide}

%%%%%%%%%%%%%%%%%%%%%%% Next Slide %%%%%%%%%%%%%%%%%%%%%%%

\begin{slide}
\section[-2]{Using Dynamic Programming}
\toptarget{paragraphStuff}
\begin{PauseHighLight}
  \begin{itemize}
  \item The challenge is recognising that you can use dynamic
    programming and representing the problem right\pause
  \item Best way to see this is through examples\pause
  \item Consider writing a word processor that splits paragraphs up into
    lines\pause
  \item You want to choose the line breaks so that the lines are all
    roughly the same length\pause
  \item This is a global optimisation task
    \begin{quote}
      \textit{minimise the total number of spaces left at the end of
        each line}\pause
    \end{quote}
  \end{itemize}
\end{PauseHighLight}

\end{slide}

%%%%%%%%%%%%%%%%%%%%%%% Next Slide %%%%%%%%%%%%%%%%%%%%%%%

\begin{slide}
\section{Using Dynamic Programming}

\pb
\begin{itemize} \small
\item I have a dream that one day this nation will rise up and live out
  the\ldots \pause
\end{itemize}
\begin{center}
  \multipdf[width=\linewidth]{lineBreak}\pause
\end{center}
\end{slide}

%%%%%%%%%%%%%%%%%%%%%%% Next Slide %%%%%%%%%%%%%%%%%%%%%%%

\begin{slide}
\section{Real Word Breaking}

\begin{PauseHighLight}
  \begin{itemize}
  \item In advanced word processing you care about hyphenation, large
    gaps at the end of lines, etc.\pause
  \item These all affect the way you would assign costs\pause
  \item Dynamic programming is used in \LaTeX\ to produce nice line
    breaks\pause
  \item A similar algorithm is used to produce nice page breaks\pause
  \end{itemize}
\end{PauseHighLight}

\end{slide}

%%%%%%%%%%%%%%%%%%%%%%% Next Slide %%%%%%%%%%%%%%%%%%%%%%%

\begin{slide}
\section{Inexact Matching}
\toptarget{other}

\begin{PauseHighLight}
  \begin{itemize}
  \item A second example of dynamic programming is to find inexact
    matches\pause
  \item The edit distance between two strings is the number of changes
    needed to move from one string to another\pause
  \item The exact metric depends on the application, but might include
    number of substitutions, insertions and deletions\pause
  \item This has many applications, e.g.\ in genomics to see what DNA
    strings (or proteins) are related\pause
  \end{itemize}
\end{PauseHighLight}

\end{slide}

%%%%%%%%%%%%%%%%%%%%%%% Next Slide %%%%%%%%%%%%%%%%%%%%%%%

\begin{slide}
\section[-2]{Edit Distance}

\pb
\begin{itemize}
\item What is the minimum edit distance between ACAATTC and AGCAATC\pauseh\pauselevel{=1}
\end{itemize}
\begin{center}
  \multipdf[width=0.65\linewidth]{editdistance}\pause
\end{center}
\end{slide}

%%%%%%%%%%%%%%%%%%%%%%% Next Slide %%%%%%%%%%%%%%%%%%%%%%%

\begin{slide}
\section{Dijkstra's Algorithm}
\toptarget{dijkstra}

\begin{PauseHighLight}
  \begin{itemize}
  \item We saw early Dijkstra's algorithm for find the minimum distance
    between a source and destination node\pause
  \item We grouped this with the greedy algorithms as we choose the next
    node to add to the minimum-distance spanning tree to be the closest
    node to the source we could access\pause
  \item However, we should perhaps more rightly identify it as using a
    dynamic programming strategy as we are building up the cost of
    getting of the partial solution to reach a node\pause
  \item We use the greedy strategy to ensure that we always find the
    shorter paths first\pause
  \end{itemize}
\end{PauseHighLight}

\end{slide}

%%%%%%%%%%%%%%%%%%%%%%% Next Slide %%%%%%%%%%%%%%%%%%%%%%%

\begin{slide}
\section{Going from Node 0 to Node 11}

\pb\pause\pauselevel{=1}
\begin{center}
  \multipdf[width=1\linewidth]{dijkstra}\pause
\end{center}
\end{slide}

%%%%%%%%%%%%%%%%%%%%%%% Next Slide %%%%%%%%%%%%%%%%%%%%%%%

\begin{slide}
\section[-2]{Uses of Dynamic Programming}

\begin{PauseHighLight}
  \begin{itemize}\small\squeeze
  \item Recurrent solutions to lattice models for protein-DNA binding
  \item Backward induction as a solution method for finite-horizon
    discrete-time dynamic optimization problems 
  \item Method of undetermined coefficients can be used to solve the
    Bellman equation in infinite-horizon, discrete-time, discounted,
    time-invariant dynamic optimization problems 
  \item Many string algorithms including longest common subsequence,
    longest increasing subsequence, longest common substring,
    Levenshtein distance (edit distance) 
  \item Many algorithmic problems on graphs can be solved efficiently
    for graphs of bounded treewidth or bounded clique-width by using
    dynamic programming on a tree decomposition of the graph. 
  \item The Cocke–Younger–Kasami (CYK) algorithm which determines
    whether and how a given string can be generated by a given
    context-free grammar 
  \item Knuth's word wrapping algorithm that minimizes raggedness when
    word wrapping text 
  \item The use of transposition tables and refutation tables in
    computer chess 
  \item The Viterbi algorithm (used for hidden Markov models)
  \item The Earley algorithm (a type of chart parser)
  \item The Needleman–Wunsch and other algorithms used in
    bioinformatics, including sequence alignment, structural alignment,
    RNA structure prediction 
  \item Floyd's all-pairs shortest path algorithm
  \item Optimizing the order for chain matrix multiplication
  \item Pseudo-polynomial time algorithms for the subset sum and
    knapsack and partition problems 
  \item The dynamic time warping algorithm for computing the global
    distance between two time series 
  \item The Selinger (a.k.a. System R) algorithm for relational database
    query optimization 
  \item De Boor algorithm for evaluating B-spline curves
  \item Duckworth–Lewis method for resolving the problem when games of
    cricket are interrupted 
  \item The value iteration method for solving Markov decision processes
  \item Some graphic image edge following selection methods such as the
    "magnet" selection tool in Photoshop 
  \item Some methods for solving interval scheduling problems
  \item Some methods for solving word wrap problems
  \item Some methods for solving the travelling salesman problem, either
    exactly (in exponential time) or approximately (e.g. via the bitonic
    tour) 
  \item Recursive least squares method
  \item Beat tracking in music information retrieval
  \item Adaptive-critic training strategy for artificial neural networks
  \item Stereo algorithms for solving the correspondence problem used in
    stereo vision 
  \item Seam carving (content aware image resizing)
  \item The Bellman–Ford algorithm for finding the shortest distance in
    a graph 
  \item Some approximate solution methods for the linear search problem
  \item Kadane's algorithm for the maximum subarray problem
  \end{itemize}
\end{PauseHighLight}

\end{slide}


%%%%%%%%%%%%%%%%%%%%%%% Next Slide %%%%%%%%%%%%%%%%%%%%%%%
\Outline % Limitations
%%%%%%%%%%%%%%%%%%%%%%% Next Slide %%%%%%%%%%%%%%%%%%%%%%%

\begin{slide}
\section{When You Can't Use It}

\begin{PauseHighLight}
  \begin{itemize}
  \item Not all problems can be split neatly to make dynamic programming
    possible\pause
  \item Dynamic programming works on problems with some natural
    ordering\pause 
  \item We need this to build up a list of optimum cost of partial
    solutions---these have to depend on the cost of previous partial
    solutions\pause
  \item Sometime no natural ordering exists\pause
  \end{itemize}
\end{PauseHighLight}

\end{slide}

%%%%%%%%%%%%%%%%%%%%%%% Next Slide %%%%%%%%%%%%%%%%%%%%%%%

\begin{slide}
\section[-2]{Travelling Salesman Problem}

\pb
\begin{itemize}
\item For TSP we can find a dynamic programming solution by
  considering optimal sub-tours (paths) involving sets of $k$
  cities\pauseh
\item If we know the optimal sub-tour through all sets of cities of
  size $k$ (starting and finishing at each possible pair in the set)
  then we can quickly compute the optimal sub-tour between set of size
  $k+1$\pauseh
  \begin{center}
    \multipdf[width=0.5\linewidth]{tsp_dp}\pause  
  \end{center}
\end{itemize}



\end{slide}

%%%%%%%%%%%%%%%%%%%%%%% Next Slide %%%%%%%%%%%%%%%%%%%%%%%

\begin{slide}
\section{Travelling Salesman Problem}

\begin{PauseHighLight}
  \begin{itemize}
  \item The problem is there are $\binom{n}{k}$ subsets consisting of
    $k$ cities out of a possible $n$\pause
  \item The total number of subsets that need to be considered is
    \begin{align*}
      \sum_{k=0}^n \binom{n}{k} = 2^n\pause
    \end{align*}
  \item The time complexity of the DP solution is $n^2\,2^n$ which is
    better than $n!$ and is currently the fastest known exact algorithm for
    TSP\pause, but it ain't very useful in practice!\pauseb
  \end{itemize}
\end{PauseHighLight}


\end{slide}



%%%%%%%%%%%%%%%%%%%%%%% Next Slide %%%%%%%%%%%%%%%%%%%%%%%

\begin{slide}
\section{Conclusions}

\begin{PauseHighLight}
  \begin{itemize}
  \item Dynamic programming is one of the most powerful strategies for
    solving hard optimisation problems\pause
  \item It works by iteratively building up costs for partial solutions
    using the costs of smaller partial solutions\pause
  \item When it works it is great and there are hosts of practical
    algorithms which use DP\pause
  \item However, it doesn't always work\pause
  \end{itemize}
\end{PauseHighLight}

\end{slide}
